{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "# Prepare the different parts of the data path\n",
    "raw_layer_base_path = 'dbfs:/Volumes/emissions_datapipeline_workspace/default/datalake/raw'\n",
    "co2_emissions_feed  = 'co2_passenger_cars_emissions'\n",
    "\n",
    "spark = DatabricksSession.builder.profile(\"DEFAULT\").getOrCreate()\n",
    "\n",
    "# Read the data using Spark\n",
    "df_co2_emissions = (spark.read\n",
    "                         .option('multiline','true')\n",
    "                         .json(f'{raw_layer_base_path}/{co2_emissions_feed}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Column names:: ['At1 (mm)', 'At2 (mm)', 'Cn', 'Cr', 'Ct', 'De', 'E (g/km)', 'Enedc (g/km)', 'Er (g/km)', 'Ernedc (g/km)', 'Erwltp (g/km)', 'Ewltp (g/km)', 'Fm', 'Ft', 'ID', 'It', 'MMS', 'MS', 'Man', 'Mh', 'Mk', 'Mp', 'Mt', 'Status', 'T', 'TAN', 'VFN', 'Va', 'Ve', 'Vf', 'W (mm)', 'Zr', 'ec (cm3)', 'ep (KW)', 'm (kg)', 'r', 'version_file', 'year', 'z (Wh/km)']\n",
      "Updated Column names:: ['At1_mm', 'At2_mm', 'Cn', 'Cr', 'Ct', 'De', 'E_g/km', 'Enedc_g/km', 'Er_g/km', 'Ernedc_g/km', 'Erwltp_g/km', 'Ewltp_g/km', 'Fm', 'Ft', 'ID', 'It', 'MMS', 'MS', 'Man', 'Mh', 'Mk', 'Mp', 'Mt', 'Status', 'T', 'TAN', 'VFN', 'Va', 'Ve', 'Vf', 'W_mm', 'Zr', 'ec_cm3', 'ep_KW', 'm_kg', 'r', 'version_file', 'year', 'z_Wh/km']\n"
     ]
    }
   ],
   "source": [
    "# Replace spaces in column names with underscores (“_”). Additionally, remove parentheses from column names.\n",
    "import re\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "co2_emissions_columns = df_co2_emissions.columns\n",
    "print(f\"Original Column names:: {df_co2_emissions.columns}\")\n",
    "\n",
    "df_co2_emissions = (df_co2_emissions.select(\n",
    "                      [F.col(col).alias(re.sub('[()]', '', col.replace(' ', '_'))) for col in df_co2_emissions.columns]\n",
    "                    ))\n",
    "\n",
    "\n",
    "print(f\"Updated Column names:: {df_co2_emissions.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count prior to dropping null values:: 300000\n",
      "Record count after to dropping null values:: 300000\n"
     ]
    }
   ],
   "source": [
    "# Drop records that only consist of null values (records with null values on all columns).\n",
    "print(f\"Record count prior to dropping null values:: {df_co2_emissions.count()}\")\n",
    "df_co2_emissions = df_co2_emissions.dropna(how=\"all\")\n",
    "print(f\"Record count after to dropping null values:: {df_co2_emissions.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count prior to dropping duplicate values:: 300000\n",
      "Record count after to dropping duplicate values:: 300000\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate records.\n",
    "\n",
    "print(f\"Record count prior to dropping duplicate values:: {df_co2_emissions.count()}\")\n",
    "df_co2_emissions = df_co2_emissions.dropDuplicates()\n",
    "print(f\"Record count after to dropping duplicate values:: {df_co2_emissions.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count prior to filtered state code:: 300000\n",
      "Record count with filtered state code:: 299996\n"
     ]
    }
   ],
   "source": [
    "# Drop all records that have a member state code size other than two (column: MS) and that contain any character other than uppercase letters in this column\n",
    "\n",
    "print(f\"Record count prior to filtered state code:: {df_co2_emissions.count()}\")\n",
    "df_co2_emissions = df_co2_emissions.filter(df_co2_emissions['MS'].rlike('^[A-Z][A-Z]$'))\n",
    "print(f\"Record count with filtered state code:: {df_co2_emissions.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_year = datetime.now().year\n",
    "\n",
    "dbfs_wdi_data_path = f\"dbfs:/Volumes/emissions_datapipeline_workspace/default/datalake/curated/co2_emissions/year={current_year}/\"\n",
    "df_co2_emissions = df_co2_emissions.repartition('year')\n",
    "\n",
    "df_co2_emissions.write.mode(\"overwrite\").partitionBy('year').parquet(dbfs_wdi_data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
