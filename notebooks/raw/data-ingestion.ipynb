{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Catalog Setup\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Running\")\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "print(\"current catalog:\", spark.catalog.currentCatalog())\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the SQL script\n",
    "sql_script = \"\"\"\n",
    "CREATE EXTERNAL CATALOG 'iceberg'\n",
    "PROPERTIES\n",
    "(\n",
    "  \"type\"=\"iceberg\",\n",
    "  \"iceberg.catalog.type\"=\"rest\",\n",
    "  \"iceberg.catalog.uri\"=\"http://iceberg-rest:8181\",\n",
    "  \"iceberg.catalog.warehouse\"=\"warehouse\",\n",
    "  \"aws.s3.access_key\"=\"admin\",\n",
    "  \"aws.s3.secret_key\"=\"password\",\n",
    "  \"aws.s3.endpoint\"=\"http://minio:9000\",\n",
    "  \"aws.s3.enable_path_style_access\"=\"true\",\n",
    "  \"client.factory\"=\"com.starrocks.connector.iceberg.IcebergAwsClientFactory\"\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Save the SQL script to a temporary file\n",
    "with open(\"/tmp/script.sql\", \"w\") as f:\n",
    "    f.write(sql_script)\n",
    "\n",
    "# Run the SQL script using docker compose and StarRocks\n",
    "command = [\n",
    "    \"mysql\", \"-P\", \"9030\", \"-h\", \"starrocks-fe\", \"-u\", \"root\", \"--prompt=StarRocks > \", \n",
    "    \"-e\", \"source /tmp/script.sql\"\n",
    "]\n",
    "\n",
    "# Execute the command\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the output\n",
    "print(result.stdout)\n",
    "print(result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        for line in lines:\n",
    "            file.write(line.rstrip().rstrip(',') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "wbi_data_path = \"/home/iceberg/data/world_bank_data\"\n",
    "csv_files = [file for file in os.listdir(wbi_data_path) if file.endswith(\".csv\")]\n",
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS raw\")\n",
    "\n",
    "for csv in csv_files:\n",
    "  file_path = os.path.join(wbi_data_path, csv)\n",
    "  clean_csv(file_path)\n",
    "  file_name = Path(file_path).stem\n",
    "  file_name = file_name.replace(\"-\", \"_\")\n",
    "\n",
    "  df = spark.read.option('header', 'true').csv(file_path)\n",
    "\n",
    "  (df\n",
    "  .writeTo(f\"raw.world_development_indicators.{file_name}\")\n",
    "  .options(format=\"iceberg\", mode=\"overwrite\")\n",
    "  .createOrReplace()\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_data_path = \"/home/iceberg/data/emissions_data\"\n",
    "\n",
    "for year in [2017, 2018, 2019]:\n",
    "  file_path = f\"{emissions_data_path}/co2_emissions_passenger_cars_{year}.json\"\n",
    "  file_name = Path(file_path).stem\n",
    "\n",
    "  df = spark.read.option(\"multiline\",\"true\").json(file_path)\n",
    "\n",
    "  (df\n",
    "  .writeTo(f\"raw.co2_passenger_cars_emissions.{file_name}\")\n",
    "  .options(format=\"iceberg\", mode=\"overwrite\")\n",
    "  .createOrReplace()\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_data_df = spark.read.table(\"iceberg.raw.world_development_indicators.WDIData\")\n",
    "iceberg_co2_emissions_df = spark.read.table(\"iceberg.raw.co2_passenger_cars_emissions.co2_emissions_passenger_cars_2017\")\n",
    "\n",
    "iceberg_data_df.printSchema()\n",
    "iceberg_co2_emissions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of records  for CO2 emissions DF: {iceberg_co2_emissions_df.count()}\")\n",
    "print(f\"Number of records  for World Development Indicators: {iceberg_data_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(iceberg_co2_emissions_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(iceberg_data_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(iceberg_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(iceberg_co2_emissions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
