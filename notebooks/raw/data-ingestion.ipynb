{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Catalog Setup\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Running\")\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "print(\"current catalog:\", spark.catalog.currentCatalog())\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        for line in lines:\n",
    "            file.write(line.rstrip().rstrip(',') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "wbi_data_path = \"/home/iceberg/data/world_bank_data\"\n",
    "csv_files = [file for file in os.listdir(wbi_data_path) if file.endswith(\".csv\")]\n",
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS raw\")\n",
    "\n",
    "for csv in csv_files:\n",
    "  file_path = os.path.join(wbi_data_path, csv)\n",
    "  clean_csv(file_path)\n",
    "  file_name = Path(file_path).stem\n",
    "  file_name = file_name.replace(\"-\", \"_\")\n",
    "\n",
    "  df = spark.read.option('header', 'true').csv(file_path)\n",
    "\n",
    "  (df\n",
    "  .write\n",
    "  .mode('overwrite')\n",
    "  .format('iceberg')\n",
    "  .saveAsTable(f'raw.world_development_indicators.{file_name}')\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_data_path = \"/home/iceberg/data/emissions_data\"\n",
    "\n",
    "for year in [2017, 2018, 2019]:\n",
    "  file_path = f\"{emissions_data_path}/co2_emissions_passenger_cars_{year}.json\"\n",
    "  file_name = Path(file_path).stem\n",
    "\n",
    "  df = spark.read.option(\"multiline\",\"true\").json(file_path)\n",
    "\n",
    "  (df\n",
    "  .write\n",
    "  .mode('overwrite')\n",
    "  .format('iceberg')\n",
    "  .saveAsTable(f'raw.co2_passenger_cars_emissions.{file_name}')\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_data_df = spark.read.table(\"raw.world_development_indicators.WDIData\")\n",
    "iceberg_co2_emissions_df = spark.read.table(\"raw.co2_passenger_cars_emissions.co2_emissions_passenger_cars_2017\")\n",
    "\n",
    "iceberg_data_df.printSchema()\n",
    "iceberg_co2_emissions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of records  for CO2 emissions DF: {iceberg_co2_emissions_df.count()}\")\n",
    "print(f\"Number of records  for World Development Indicators: {iceberg_data_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(iceberg_co2_emissions_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(iceberg_data_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(iceberg_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(iceberg_co2_emissions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = spark.catalog.listDatabases()\n",
    "\n",
    "# Print database names\n",
    "for db in databases:\n",
    "    print(db.name)\n",
    "\n",
    "print(\"current catalog:\", spark.catalog.currentCatalog())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
