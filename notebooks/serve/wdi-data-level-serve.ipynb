{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 19:17:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n",
      "[('spark.eventLog.enabled', 'true'), ('spark.driver.cores', '4'), ('spark.driver.host', '30c3b29e5e01'), ('spark.driver.port', '39021'), ('spark.task.cpus', '4'), ('spark.app.startTime', '1721157426132'), ('spark.executor.cores', '4'), ('spark.history.fs.logDirectory', '/home/iceberg/spark-events'), ('spark.sql.catalog.demo.s3.endpoint', 'http://minio:9000'), ('spark.eventLog.dir', '/home/iceberg/spark-events'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.executor.memory', '8g'), ('spark.submit.deployMode', 'client'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalogImplementation', 'in-memory'), ('spark.app.submitTime', '1721157426008'), ('spark.sql.catalog.demo.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO'), ('spark.sql.catalog.demo.warehouse', 's3://warehouse/wh/'), ('spark.executor.id', 'driver'), ('spark.app.id', 'local-1721157426659'), ('spark.app.name', 'PySparkShell'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.sql.catalog.demo.uri', 'http://rest:8181'), ('spark.sql.catalog.demo.type', 'rest'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalog.demo', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.defaultCatalog', 'demo'), ('spark.sql.warehouse.dir', 'file:/home/iceberg/notebooks/spark-warehouse'), ('spark.submit.pyFiles', ''), ('spark.ui.showConsoleProgress', 'true')]\n",
      "current catalog: demo\n",
      "Spark UI: http://30c3b29e5e01:4042\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Catalog Setup\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://iceberg-rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"warehouse\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.access-key\", \"admin\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.secret-key\", \"password\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.client.factory\", \"com.starrocks.connector.iceberg.IcebergAwsClientFactory\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Running\")\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "print(\"current catalog:\", spark.catalog.currentCatalog())\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the WDI unpivoted data\n",
    "df_wdi_unpivoted = spark.read.table(\"wdi_serving.wdi_data_unpivoted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:00:12.235306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Python function to generate the decade\n",
    "# This can be done in multiple ways\n",
    "# One way is to simply keep the first three chars from the year\n",
    "# and then add '0s'\n",
    "def generate_decade_value(year: int) -> str:\n",
    "  return str(year)[:3] + '0s'\n",
    "\n",
    "# We register the function as a UDF\n",
    "# We can also use the @udf annotation\n",
    "generate_decade_udf = udf(generate_decade_value)\n",
    "\n",
    "# We add the decade column\n",
    "df_wdi_unpivoted_udf_v1 = df_wdi_unpivoted.withColumn('decade', generate_decade_udf('year'))\n",
    "\n",
    "\n",
    "# we start the timer\n",
    "start = datetime.now()\n",
    "\n",
    "# We use noop format to simulate the write action\n",
    "(df_wdi_unpivoted_udf_v1\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .format(\"noop\")\n",
    " .save())\n",
    "\n",
    "# We print the time taken by our job\n",
    "print(f'Time taken: {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=================================================>       (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:00:04.358346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# This time we're using the annotation\n",
    "# The Pandas UDF input and output is of type pd.Series\n",
    "# We use a map function on the input Series to perform the modification\n",
    "@pandas_udf(StringType())\n",
    "def generate_decade_udf_v2(year: pd.Series) -> pd.Series:\n",
    "  return year.map(lambda x: str(x)[:3] + '0s')\n",
    "\n",
    "\n",
    "# We add the decade column\n",
    "df_wdi_unpivoted_udf_v2 = df_wdi_unpivoted.withColumn('decade', generate_decade_udf_v2('year'))\n",
    "\n",
    "\n",
    "# we start the timer\n",
    "start = datetime.now()\n",
    "\n",
    "# We use noop format to simulate the write action\n",
    "(df_wdi_unpivoted_udf_v2\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .format(\"noop\")\n",
    " .save())\n",
    "\n",
    "# We print the time taken by our job\n",
    "print(f'Time taken: {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas UDF is faster mainly because it leverages [Apache Arrow](https://arrow.apache.org/) for the data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "  df_wdi_unpivoted_udf_v2\n",
    "  .repartition('decade')\n",
    "  .write\n",
    "  .mode('overwrite')\n",
    "  .format('iceberg')\n",
    "  .partitionBy('decade')\n",
    "  .saveAsTable('wdi_serving.decade_level_data')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+-----------------+-----------------+----+------+\n",
      "|        Country_Name|Country_Code|      Indicator_Name|   Indicator_Code|  Indicator_Value|year|decade|\n",
      "+--------------------+------------+--------------------+-----------------+-----------------+----+------+\n",
      "|Africa Eastern an...|         AFE|Age dependency ra...|   SP.POP.DPND.OL| 5.59001872292559|2012| 2010s|\n",
      "|Caribbean small s...|         CSS|   GDP (current US$)|   NY.GDP.MKTP.CD| 72051116148.0838|2012| 2010s|\n",
      "|East Asia & Pacif...|         EAP|Population ages 0...|SP.POP.0014.FE.ZS|  20.522287762446|2012| 2010s|\n",
      "|           Euro area|         EMU|  Population, female|SP.POP.TOTL.FE.IN|        172120855|2012| 2010s|\n",
      "|Europe & Central ...|         ECS|Population ages 5...|SP.POP.5054.FE.5Y| 6.92774019899506|2012| 2010s|\n",
      "|Fragile and confl...|         FCS|Population ages 8...|SP.POP.80UP.MA.5Y|0.349797743847993|2012| 2010s|\n",
      "|         High income|         HIC|Age dependency ra...|   SP.POP.DPND.YG| 25.5308716957404|2012| 2010s|\n",
      "|           IDA total|         IDA|GDP (constant 201...|   NY.GDP.MKTP.KD| 1618868279733.74|2012| 2010s|\n",
      "|Latin America & t...|         TLA|GDP per capita (c...|   NY.GDP.PCAP.KD|  9446.1301391853|2012| 2010s|\n",
      "|Least developed c...|         LDC|Population ages 6...|SP.POP.65UP.TO.ZS| 3.43370350805365|2012| 2010s|\n",
      "+--------------------+------------+--------------------+-----------------+-----------------+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_wdi_unpivoted_udf_v2.show(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
