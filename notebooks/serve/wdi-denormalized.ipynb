{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n",
      "[('spark.eventLog.enabled', 'true'), ('spark.driver.cores', '4'), ('spark.task.cpus', '4'), ('spark.executor.cores', '4'), ('spark.history.fs.logDirectory', '/home/iceberg/spark-events'), ('spark.sql.catalog.demo.s3.endpoint', 'http://minio:9000'), ('spark.driver.port', '32891'), ('spark.eventLog.dir', '/home/iceberg/spark-events'), ('spark.app.id', 'local-1721266203718'), ('spark.app.startTime', '1721266203673'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.executor.memory', '8g'), ('spark.submit.deployMode', 'client'), ('spark.driver.host', '2f37682e1403'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalogImplementation', 'in-memory'), ('spark.sql.catalog.demo.warehouse', 's3://warehouse/wh/'), ('spark.sql.catalog.demo.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO'), ('spark.app.submitTime', '1721265897826'), ('spark.executor.id', 'driver'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.sql.catalog.demo.uri', 'http://rest:8181'), ('spark.sql.catalog.demo.type', 'rest'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalog.demo', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.defaultCatalog', 'demo'), ('spark.submit.pyFiles', ''), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.name', 'Iceberg Catalog Setup')]\n",
      "current catalog: demo\n",
      "Spark UI: http://2f37682e1403:4041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/18 01:30:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Catalog Setup\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Running\")\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "print(\"current catalog:\", spark.catalog.currentCatalog())\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_data_df = spark.read.table(\"curated.world_development_indicators.data\")\n",
    "wdi_country_df = spark.read.table(\"curated.world_development_indicators.country\")\n",
    "wdi_series_df = spark.read.table(\"curated.world_development_indicators.series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter the countries dataframe to keep data that references actual countries.\n",
    "# To do so, we filter on the Region column.\n",
    "df_wdi_countries_filtered = (wdi_country_df\n",
    "                             .where('Region is not Null')\n",
    "                             .select(\n",
    "                               'Country_Code',\n",
    "                               '2-alpha_code',\n",
    "                               'Currency_Unit',\n",
    "                               'Region',\n",
    "                               'Income_Group'\n",
    "                            ))\n",
    "\n",
    "# Perform an inner join to keep only countries' data in the main dataframe\n",
    "# and add all the columns that come from the countries dataframe.\n",
    "df_wdi_country_data = (wdi_data_df\n",
    "                       .join(\n",
    "                         df_wdi_countries_filtered,\n",
    "                         on = ['Country_Code'],\n",
    "                         how = 'inner'\n",
    "                       ))\n",
    "\n",
    "# Write the output dataframe to the serving layer\n",
    "# And we create an external table on top of it\n",
    "\n",
    "(df_wdi_country_data\n",
    ".repartition('Country_Code')\n",
    ".writeTo(\"wdi_serving.countries_data\")\n",
    ".partitionedBy(\"Country_Code\")\n",
    ".options(format=\"iceberg\", mode=\"overwrite\")\n",
    ".createOrReplace()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co2_emissions_2017 = spark.read.table(\"curated.co2_passenger_cars_emissions\").filter(\"year = 2017\")\n",
    "\n",
    "df_co2_emissions_2018 = spark.read.table(\"curated.co2_passenger_cars_emissions\").filter(\"year = 2018\")\n",
    "\n",
    "df_co2_emissions_2019 = spark.read.table(\"curated.co2_passenger_cars_emissions\").filter(\"year = 2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Group the data using the MS column\n",
    "# And then we sum the values of the Enedc_g/km column\n",
    "df_co2_emissions_2017 = (df_co2_emissions_2017\n",
    "                         .groupBy('MS')\n",
    "                         .agg(\n",
    "                           F.sum('Enedc_g/km').alias('sum_2017')\n",
    "                         )\n",
    "                        )\n",
    "df_co2_emissions_2018 = (df_co2_emissions_2018\n",
    "                         .groupBy('MS')\n",
    "                         .agg(\n",
    "                           F.sum('Enedc_g/km').alias('sum_2018')\n",
    "                         )\n",
    "                        )\n",
    "df_co2_emissions_2019 = (df_co2_emissions_2019\n",
    "                         .groupBy('MS')\n",
    "                         .agg(\n",
    "                           F.sum('Enedc_g/km').alias('sum_2019')\n",
    "                         )\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the two dataframes that contain the YoY diff\n",
    "# Do an inner join to only keep actual diffs\n",
    "# Use withColumn() to first add the YoY diff column\n",
    "# Use withColumn() again to add a column containing the current year value\n",
    "df_diff_2018_2017 = (df_co2_emissions_2018\n",
    "                     .withColumnRenamed('sum_2018', 'sum_current_year')\n",
    "                     .join(\n",
    "                       df_co2_emissions_2017.withColumnRenamed('sum_2017', 'sum_previous_year'),\n",
    "                       on = ['MS'],\n",
    "                       how = 'inner'\n",
    "                     )\n",
    "                     .withColumn('emission_diff_yoy', F.col('sum_current_year') - F.col('sum_previous_year'))\n",
    "                     .withColumn('year', F.lit('2018'))\n",
    "                    )\n",
    "df_diff_2019_2018 = (df_co2_emissions_2019\n",
    "                     .withColumnRenamed('sum_2019', 'sum_current_year')\n",
    "                     .join(\n",
    "                       df_co2_emissions_2018.withColumnRenamed('sum_2018', 'sum_previous_year'),\n",
    "                       on = ['MS'],\n",
    "                       how = 'inner'\n",
    "                     )\n",
    "                     .withColumn('emission_diff_yoy', F.col('sum_current_year') - F.col('sum_previous_year'))\n",
    "                     .withColumn('year', F.lit('2019'))\n",
    "                    )\n",
    "\n",
    "# Use union() method to generate one dataframe containing both input dataframes\n",
    "df_emissions_diff = df_diff_2018_2017.union(df_diff_2019_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(MS='LT', sum_current_year=41396, sum_previous_year=92977, emission_diff_yoy=-51581, year='2018'),\n",
       " Row(MS='FI', sum_current_year=18352, sum_previous_year=85816, emission_diff_yoy=-67464, year='2018'),\n",
       " Row(MS='RO', sum_current_year=98029, sum_previous_year=586722, emission_diff_yoy=-488693, year='2018'),\n",
       " Row(MS='NL', sum_current_year=219140, sum_previous_year=1057643, emission_diff_yoy=-838503, year='2018'),\n",
       " Row(MS='PL', sum_current_year=926631, sum_previous_year=3316037, emission_diff_yoy=-2389406, year='2018'),\n",
       " Row(MS='EE', sum_current_year=33268, sum_previous_year=280218, emission_diff_yoy=-246950, year='2018'),\n",
       " Row(MS='AT', sum_current_year=640929, sum_previous_year=466231, emission_diff_yoy=174698, year='2018'),\n",
       " Row(MS='HR', sum_current_year=49224, sum_previous_year=24368, emission_diff_yoy=24856, year='2018'),\n",
       " Row(MS='CZ', sum_current_year=393842, sum_previous_year=1203499, emission_diff_yoy=-809657, year='2018'),\n",
       " Row(MS='PT', sum_current_year=89572, sum_previous_year=299543, emission_diff_yoy=-209971, year='2018'),\n",
       " Row(MS='GB', sum_current_year=6714318, sum_previous_year=1201060, emission_diff_yoy=5513258, year='2018'),\n",
       " Row(MS='MT', sum_current_year=3590, sum_previous_year=20646, emission_diff_yoy=-17056, year='2018'),\n",
       " Row(MS='DE', sum_current_year=14041972, sum_previous_year=2914577, emission_diff_yoy=11127395, year='2018'),\n",
       " Row(MS='ES', sum_current_year=488704, sum_previous_year=303979, emission_diff_yoy=184725, year='2018'),\n",
       " Row(MS='FR', sum_current_year=1692819, sum_previous_year=5159403, emission_diff_yoy=-3466584, year='2018'),\n",
       " Row(MS='GR', sum_current_year=8942, sum_previous_year=90422, emission_diff_yoy=-81480, year='2018'),\n",
       " Row(MS='IT', sum_current_year=1159072, sum_previous_year=564858, emission_diff_yoy=594214, year='2018'),\n",
       " Row(MS='SE', sum_current_year=849810, sum_previous_year=411317, emission_diff_yoy=438493, year='2018'),\n",
       " Row(MS='BG', sum_current_year=135981, sum_previous_year=447840, emission_diff_yoy=-311859, year='2018'),\n",
       " Row(MS='SK', sum_current_year=133227, sum_previous_year=296360, emission_diff_yoy=-163133, year='2018'),\n",
       " Row(MS='LV', sum_current_year=29646, sum_previous_year=157124, emission_diff_yoy=-127478, year='2018'),\n",
       " Row(MS='HU', sum_current_year=146978, sum_previous_year=191481, emission_diff_yoy=-44503, year='2018'),\n",
       " Row(MS='SI', sum_current_year=11850, sum_previous_year=82628, emission_diff_yoy=-70778, year='2018'),\n",
       " Row(MS='CY', sum_current_year=5082, sum_previous_year=59746, emission_diff_yoy=-54664, year='2018'),\n",
       " Row(MS='IE', sum_current_year=6212, sum_previous_year=25190, emission_diff_yoy=-18978, year='2018')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_emissions_diff.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_emissions_diff\n",
    ".writeTo(\"eea_serving.emissions_diff_yoy\")\n",
    ".partitionedBy(\"year\")\n",
    ".options(format=\"iceberg\", mode=\"overwrite\")\n",
    ".createOrReplace()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
