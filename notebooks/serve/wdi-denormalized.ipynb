{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Catalog Setup\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://iceberg-rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"warehouse\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.access-key\", \"admin\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.secret-key\", \"password\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.client.factory\", \"com.starrocks.connector.iceberg.IcebergAwsClientFactory\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Running\")\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "print(\"current catalog:\", spark.catalog.currentCatalog())\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_data_df = spark.read.table(\"curated.world_development_indicators.data\")\n",
    "wdi_country_df = spark.read.table(\"curated.world_development_indicators.country\")\n",
    "wdi_series_df = spark.read.table(\"curated.world_development_indicators.series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the countries dataframe to keep data that references actual countries.\n",
    "# To do so, we filter on the Region column.\n",
    "df_wdi_countries_filtered = (wdi_country_df\n",
    "                             .where('Region is not Null')\n",
    "                             .select(\n",
    "                               'Country_Code',\n",
    "                               '2-alpha_code',\n",
    "                               'Currency_Unit',\n",
    "                               'Region',\n",
    "                               'Income_Group'\n",
    "                            ))\n",
    "\n",
    "# Perform an inner join to keep only countries' data in the main dataframe\n",
    "# and add all the columns that come from the countries dataframe.\n",
    "df_wdi_country_data = (wdi_data_df\n",
    "                       .join(\n",
    "                         df_wdi_countries_filtered,\n",
    "                         on = ['Country_Code'],\n",
    "                         how = 'inner'\n",
    "                       ))\n",
    "\n",
    "# Write the output dataframe to the serving layer\n",
    "# And we create an external table on top of it\n",
    "(df_wdi_country_data\n",
    "  .repartition('Country_Code')\n",
    "  .write\n",
    "  .format(\"iceberg\")\n",
    "  .mode(\"overwrite\")\n",
    "  .partitionBy('Country_Code')\n",
    "  .saveAsTable('wdi_serving.countries_data')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co2_emissions_2017 = spark.read.table(\"curated.co2_passenger_cars_emissions\").filter(\"year = 2017\")\n",
    "\n",
    "df_co2_emissions_2018 = spark.read.table(\"curated.co2_passenger_cars_emissions\").filter(\"year = 2018\")\n",
    "\n",
    "df_co2_emissions_2019 = spark.read.table(\"curated.co2_passenger_cars_emissions\").filter(\"year = 2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Group the data using the MS column\n",
    "# And then we sum the values of the Enedc_g/km column\n",
    "df_co2_emissions_2017 = (df_co2_emissions_2017\n",
    "                         .groupBy('MS')\n",
    "                         .agg(\n",
    "                           F.sum('Enedc_g/km').alias('sum_2017')\n",
    "                         )\n",
    "                        )\n",
    "df_co2_emissions_2018 = (df_co2_emissions_2018\n",
    "                         .groupBy('MS')\n",
    "                         .agg(\n",
    "                           F.sum('Enedc_g/km').alias('sum_2018')\n",
    "                         )\n",
    "                        )\n",
    "df_co2_emissions_2019 = (df_co2_emissions_2019\n",
    "                         .groupBy('MS')\n",
    "                         .agg(\n",
    "                           F.sum('Enedc_g/km').alias('sum_2019')\n",
    "                         )\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the two dataframes that contain the YoY diff\n",
    "# Do an inner join to only keep actual diffs\n",
    "# Use withColumn() to first add the YoY diff column\n",
    "# Use withColumn() again to add a column containing the current year value\n",
    "df_diff_2018_2017 = (df_co2_emissions_2018\n",
    "                     .withColumnRenamed('sum_2018', 'sum_current_year')\n",
    "                     .join(\n",
    "                       df_co2_emissions_2017.withColumnRenamed('sum_2017', 'sum_previous_year'),\n",
    "                       on = ['MS'],\n",
    "                       how = 'inner'\n",
    "                     )\n",
    "                     .withColumn('emission_diff_yoy', F.col('sum_current_year') - F.col('sum_previous_year'))\n",
    "                     .withColumn('year', F.lit('2018'))\n",
    "                    )\n",
    "df_diff_2019_2018 = (df_co2_emissions_2019\n",
    "                     .withColumnRenamed('sum_2019', 'sum_current_year')\n",
    "                     .join(\n",
    "                       df_co2_emissions_2018.withColumnRenamed('sum_2018', 'sum_previous_year'),\n",
    "                       on = ['MS'],\n",
    "                       how = 'inner'\n",
    "                     )\n",
    "                     .withColumn('emission_diff_yoy', F.col('sum_current_year') - F.col('sum_previous_year'))\n",
    "                     .withColumn('year', F.lit('2019'))\n",
    "                    )\n",
    "\n",
    "# Use union() method to generate one dataframe containing both input dataframes\n",
    "df_emissions_diff = df_diff_2018_2017.union(df_diff_2019_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_emissions_diff.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_emissions_diff\n",
    " .write\n",
    " .format(\"iceberg\")\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy('year')\n",
    " .saveAsTable('eea_serving.emissions_diff_yoy')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
