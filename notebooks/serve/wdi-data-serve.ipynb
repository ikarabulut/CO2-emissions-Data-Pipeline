{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/15 00:45:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n",
      "[('spark.eventLog.enabled', 'true'), ('spark.driver.cores', '4'), ('spark.task.cpus', '4'), ('spark.app.id', 'local-1721004344016'), ('spark.driver.port', '45339'), ('spark.executor.cores', '4'), ('spark.history.fs.logDirectory', '/home/iceberg/spark-events'), ('spark.sql.catalog.demo.s3.endpoint', 'http://minio:9000'), ('spark.eventLog.dir', '/home/iceberg/spark-events'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.executor.memory', '8g'), ('spark.submit.deployMode', 'client'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalogImplementation', 'in-memory'), ('spark.sql.catalog.demo.warehouse', 's3://warehouse/wh/'), ('spark.sql.catalog.demo.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO'), ('spark.app.submitTime', '1721004343299'), ('spark.app.startTime', '1721004343442'), ('spark.executor.id', 'driver'), ('spark.app.name', 'PySparkShell'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.sql.catalog.demo.uri', 'http://rest:8181'), ('spark.driver.host', '29b2b8359a34'), ('spark.sql.catalog.demo.type', 'rest'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalog.demo', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.defaultCatalog', 'demo'), ('spark.sql.warehouse.dir', 'file:/home/iceberg/notebooks/spark-warehouse'), ('spark.submit.pyFiles', ''), ('spark.ui.showConsoleProgress', 'true')]\n",
      "current catalog: demo\n",
      "Spark UI: http://29b2b8359a34:4041\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Catalog Setup\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://iceberg-rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"warehouse\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.access-key\", \"admin\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.secret-key\", \"password\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.client.factory\", \"com.starrocks.connector.iceberg.IcebergAwsClientFactory\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Running\")\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "print(\"current catalog:\", spark.catalog.currentCatalog())\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "wdi_data_df = spark.read.table(\"curated.world_development_indicators.data\")\n",
    "\n",
    "print(wdi_data_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "root\n",
      " |-- Country_Name: string (nullable = true)\n",
      " |-- Country_Code: string (nullable = true)\n",
      " |-- Indicator_Name: string (nullable = true)\n",
      " |-- Indicator_Code: string (nullable = true)\n",
      " |-- Indicator_Value: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+-----------------+----------------+----+\n",
      "|        Country_Name|Country_Code|      Indicator_Name|   Indicator_Code| Indicator_Value|year|\n",
      "+--------------------+------------+--------------------+-----------------+----------------+----+\n",
      "|Africa Eastern an...|         AFE|Age dependency ra...|   SP.POP.DPND.OL|5.80595111963956|1960|\n",
      "|Caribbean small s...|         CSS|   GDP (current US$)|   NY.GDP.MKTP.CD|1880306125.08709|1960|\n",
      "|East Asia & Pacif...|         EAP|Population ages 0...|SP.POP.0014.FE.ZS|40.1022698469607|1960|\n",
      "|           Euro area|         EMU|  Population, female|SP.POP.TOTL.FE.IN|       138020284|1960|\n",
      "|Europe & Central ...|         ECS|Population ages 5...|SP.POP.5054.FE.5Y|6.14985530006535|1960|\n",
      "|Fragile and confl...|         FCS|Population ages 8...|SP.POP.80UP.MA.5Y|0.21932284916253|1960|\n",
      "|         High income|         HIC|Age dependency ra...|   SP.POP.DPND.YG| 46.445585532069|1960|\n",
      "|           IDA total|         IDA|GDP (constant 201...|   NY.GDP.MKTP.KD|249296166242.382|1960|\n",
      "|Latin America & t...|         TLA|GDP per capita (c...|   NY.GDP.PCAP.KD|3721.00617831239|1960|\n",
      "|Least developed c...|         LDC|Population ages 6...|SP.POP.65UP.TO.ZS|2.89164299722433|1960|\n",
      "+--------------------+------------+--------------------+-----------------+----------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "schema_wdi = StructType([\n",
    "               StructField('Country_Name', StringType(), True),\n",
    "               StructField('Country_Code', StringType(), True),\n",
    "               StructField('Indicator_Name', StringType(), True),\n",
    "               StructField('Indicator_Code', StringType(), True),\n",
    "               StructField('Indicator_Value', StringType(), True),\n",
    "               StructField('year', StringType(), True)\n",
    "             ])\n",
    "\n",
    "emptyRDD              = spark.sparkContext.emptyRDD()\n",
    "df_wdi_data_unpivoted = spark.createDataFrame(emptyRDD,schema_wdi)\n",
    "\n",
    "# We loop through the years\n",
    "# And then add the data of each year to the unpivoted dataframe\n",
    "for year in range(1960, 2021):\n",
    "  df_temp = (wdi_data_df\n",
    "             .select(\n",
    "               'Country_Name',\n",
    "               'Country_Code', \n",
    "               'Indicator_Name', \n",
    "               'Indicator_Code',\n",
    "               # We keep the column of the current year in the loop\n",
    "               F.col(str(year)).alias('Indicator_Value')\n",
    "             )\n",
    "             .withColumn('year', F.lit(year)) # We add a column that contains the value of the year\n",
    "            )\n",
    "  # We append this year's data to the output dataframe via union()\n",
    "  df_wdi_data_unpivoted = df_wdi_data_unpivoted.union(df_temp)\n",
    "\n",
    "# Printing the number of partitions of the output dataframe\n",
    "print(df_wdi_data_unpivoted.rdd.getNumPartitions())\n",
    "\n",
    "df_wdi_data_unpivoted.printSchema()\n",
    "\n",
    "df_wdi_data_unpivoted.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/15 00:45:54 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write unpivoted dataframe to a new table partitioned by year\n",
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS wdi_serving\")\n",
    "df_wdi_data_unpivoted.createOrReplaceTempView(\"data_unpivoted_tempTable\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS wdi_serving.wdi_data_unpivoted \n",
    "  USING iceberg\n",
    "  PARTITIONED BY (year) \n",
    "  AS SELECT * FROM data_unpivoted_tempTable\n",
    "\"\"\")\n",
    "\n",
    "spark.catalog.dropTempView(\"data_unpivoted_tempTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Country_Name: string, Country_Code: string, Indicator_Name: string, Indicator_Code: string, Indicator_Value: string, year: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use agg() method to perform aggregations\n",
    "# We use avg() from the pyspark.sql.functions module to generate the average\n",
    "# We apply the avg() function on a column from the grouped dataframe\n",
    "df_wdi_data_average = (df_wdi_data_unpivoted\n",
    "                       .groupBy(\n",
    "                         'Country_Name',\n",
    "                         'Country_Code', \n",
    "                         'Indicator_Name', \n",
    "                         'Indicator_Code',\n",
    "                       )\n",
    "                       .agg(\n",
    "                        F.avg('Indicator_Value').alias('Indicator_Average_Value')\n",
    "                       )\n",
    "                      )\n",
    "\n",
    "df_wdi_data_unpivoted.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Writing the output data to the serving layer on DBFS\n",
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS wdi_serving\")\n",
    "repartitioned_df_wdi_data_average = df_wdi_data_average.repartition('Indicator_Code')\n",
    "\n",
    "(repartitioned_df_wdi_data_average\n",
    " .repartition(\"Indicator_Code\")\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .format('iceberg')\n",
    " .partitionBy('Indicator_Code')\n",
    " .saveAsTable('wdi_serving.partitioned_average_indicators')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
