{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/23 19:07:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n",
      "[('spark.eventLog.enabled', 'true'), ('spark.driver.cores', '4'), ('spark.task.cpus', '4'), ('spark.executor.cores', '4'), ('spark.history.fs.logDirectory', '/home/iceberg/spark-events'), ('spark.app.submitTime', '1721761671044'), ('spark.sql.catalog.demo.s3.endpoint', 'http://minio:9000'), ('spark.eventLog.dir', '/home/iceberg/spark-events'), ('spark.app.id', 'local-1721761671815'), ('spark.sql.warehouse.dir', 'file:/home/iceberg/notebooks/notebooks/serve/spark-warehouse'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.executor.memory', '8g'), ('spark.submit.deployMode', 'client'), ('spark.driver.host', 'b9c7b6f17546'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalogImplementation', 'in-memory'), ('spark.sql.catalog.demo.warehouse', 's3://warehouse/wh/'), ('spark.sql.catalog.demo.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO'), ('spark.executor.id', 'driver'), ('spark.app.name', 'PySparkShell'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.sql.catalog.demo.uri', 'http://rest:8181'), ('spark.sql.catalog.demo.type', 'rest'), ('spark.driver.port', '38507'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalog.demo', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.defaultCatalog', 'demo'), ('spark.submit.pyFiles', ''), ('spark.app.startTime', '1721761671209'), ('spark.ui.showConsoleProgress', 'true')]\n",
      "current catalog: demo\n",
      "Spark UI: http://b9c7b6f17546:4041\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Catalog Setup\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Running\")\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "print(\"current catalog:\", spark.catalog.currentCatalog())\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "wdi_data_df = spark.read.table(\"curated.world_development_indicators.data\")\n",
    "\n",
    "print(wdi_data_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "root\n",
      " |-- Country_Name: string (nullable = true)\n",
      " |-- Country_Code: string (nullable = true)\n",
      " |-- Indicator_Name: string (nullable = true)\n",
      " |-- Indicator_Code: string (nullable = true)\n",
      " |-- Indicator_Value: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+-----------------+----------------+----+\n",
      "|        Country_Name|Country_Code|      Indicator_Name|   Indicator_Code| Indicator_Value|year|\n",
      "+--------------------+------------+--------------------+-----------------+----------------+----+\n",
      "|Africa Eastern an...|         AFE|Age dependency ra...|   SP.POP.DPND.OL|5.80595111963956|1960|\n",
      "|Caribbean small s...|         CSS|   GDP (current US$)|   NY.GDP.MKTP.CD|1880306125.08709|1960|\n",
      "|East Asia & Pacif...|         EAP|Population ages 0...|SP.POP.0014.FE.ZS|40.1022698469607|1960|\n",
      "|           Euro area|         EMU|  Population, female|SP.POP.TOTL.FE.IN|       138020284|1960|\n",
      "|Europe & Central ...|         ECS|Population ages 5...|SP.POP.5054.FE.5Y|6.14985530006535|1960|\n",
      "|Fragile and confl...|         FCS|Population ages 8...|SP.POP.80UP.MA.5Y|0.21932284916253|1960|\n",
      "|         High income|         HIC|Age dependency ra...|   SP.POP.DPND.YG| 46.445585532069|1960|\n",
      "|           IDA total|         IDA|GDP (constant 201...|   NY.GDP.MKTP.KD|249296166242.382|1960|\n",
      "|Latin America & t...|         TLA|GDP per capita (c...|   NY.GDP.PCAP.KD|3721.00617831239|1960|\n",
      "|Least developed c...|         LDC|Population ages 6...|SP.POP.65UP.TO.ZS|2.89164299722433|1960|\n",
      "+--------------------+------------+--------------------+-----------------+----------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "schema_wdi = StructType([\n",
    "               StructField('Country_Name', StringType(), True),\n",
    "               StructField('Country_Code', StringType(), True),\n",
    "               StructField('Indicator_Name', StringType(), True),\n",
    "               StructField('Indicator_Code', StringType(), True),\n",
    "               StructField('Indicator_Value', StringType(), True),\n",
    "               StructField('year', StringType(), True)\n",
    "             ])\n",
    "\n",
    "emptyRDD              = spark.sparkContext.emptyRDD()\n",
    "df_wdi_data_unpivoted = spark.createDataFrame(emptyRDD,schema_wdi)\n",
    "\n",
    "# We loop through the years\n",
    "# And then add the data of each year to the unpivoted dataframe\n",
    "for year in range(1960, 2021):\n",
    "  df_temp = (wdi_data_df\n",
    "             .select(\n",
    "               'Country_Name',\n",
    "               'Country_Code', \n",
    "               'Indicator_Name', \n",
    "               'Indicator_Code',\n",
    "               # We keep the column of the current year in the loop\n",
    "               F.col(str(year)).alias('Indicator_Value')\n",
    "             )\n",
    "             .withColumn('year', F.lit(year)) # We add a column that contains the value of the year\n",
    "            )\n",
    "  # We append this year's data to the output dataframe via union()\n",
    "  df_wdi_data_unpivoted = df_wdi_data_unpivoted.union(df_temp)\n",
    "\n",
    "# Printing the number of partitions of the output dataframe\n",
    "print(df_wdi_data_unpivoted.rdd.getNumPartitions())\n",
    "\n",
    "df_wdi_data_unpivoted.printSchema()\n",
    "\n",
    "df_wdi_data_unpivoted.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write unpivoted dataframe to a new table partitioned by year\n",
    "\n",
    "(df_wdi_data_unpivoted\n",
    ".writeTo(\"wdi_serving.wdi_data_unpivoted\")\n",
    ".partitionedBy(\"year\")\n",
    ".options(format=\"iceberg\", mode=\"overwrite\")\n",
    ".createOrReplace()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Country_Name: string, Country_Code: string, Indicator_Name: string, Indicator_Code: string, Indicator_Value: string, year: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use agg() method to perform aggregations\n",
    "# We use avg() from the pyspark.sql.functions module to generate the average\n",
    "# We apply the avg() function on a column from the grouped dataframe\n",
    "df_wdi_data_average = (df_wdi_data_unpivoted\n",
    "                       .groupBy(\n",
    "                         'Country_Name',\n",
    "                         'Country_Code', \n",
    "                         'Indicator_Name', \n",
    "                         'Indicator_Code',\n",
    "                       )\n",
    "                       .agg(\n",
    "                        F.avg('Indicator_Value').alias('Indicator_Average_Value')\n",
    "                       )\n",
    "                      )\n",
    "\n",
    "df_wdi_data_unpivoted.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(df_wdi_data_average\n",
    ".repartition(\"Indicator_Code\")\n",
    ".writeTo(\"wdi_serving.partitioned_average_indicators\")\n",
    ".partitionedBy(\"Indicator_Code\")\n",
    ".options(format=\"iceberg\", mode=\"overwrite\")\n",
    ".createOrReplace()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co2_emissions= spark.read.table(\"curated.co2_passenger_cars_emissions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To retrieve the 100 vehicles with highest emissions per year per country\n",
    "# We can use a window function\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "\n",
    "# We define our window\n",
    "# We're partitioning by member state and by year\n",
    "# We're then ordering by emissions in descending order using NEDC test\n",
    "window_emissions  = Window.partitionBy(\"year\", \"MS\").orderBy(desc(\"Enedc_g/km\"))\n",
    "\n",
    "# We add an order column based on the window we just defined\n",
    "# And then we filter based on that column to retrieve the top 100 rows\n",
    "# Finally, we drop the order column\n",
    "df_co2_ordered = (df_co2_emissions\n",
    "                  .withColumn(\"order\", row_number().over(window_emissions))\n",
    "                  .where(\"order <= 100\")\n",
    "                  .drop(\"order\")\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriterV2 at 0xffff6be64190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_co2_ordered\n",
    " .writeTo(\"eea_serving.highest_emissions\")\n",
    " .options(mode='overwrite', format=\"iceberg\")\n",
    " .partitionedBy(\"year\", \"MS\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
