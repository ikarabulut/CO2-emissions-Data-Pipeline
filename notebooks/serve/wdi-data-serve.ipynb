{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n",
      "[('spark.eventLog.enabled', 'true'), ('spark.driver.port', '37227'), ('spark.app.submitTime', '1720970383212'), ('spark.history.fs.logDirectory', '/home/iceberg/spark-events'), ('spark.sql.catalog.demo.s3.endpoint', 'http://minio:9000'), ('spark.eventLog.dir', '/home/iceberg/spark-events'), ('spark.app.startTime', '1720970383329'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.deployMode', 'client'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalogImplementation', 'in-memory'), ('spark.sql.catalog.demo.warehouse', 's3://warehouse/wh/'), ('spark.sql.catalog.demo.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO'), ('spark.executor.id', 'driver'), ('spark.app.name', 'PySparkShell'), ('spark.app.id', 'local-1720970384054'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.sql.catalog.demo.uri', 'http://rest:8181'), ('spark.sql.catalog.demo.type', 'rest'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalog.demo', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.defaultCatalog', 'demo'), ('spark.sql.warehouse.dir', 'file:/home/iceberg/notebooks/spark-warehouse'), ('spark.submit.pyFiles', ''), ('spark.driver.host', '4d9476ae0ffd'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/14 15:19:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Catalog Setup\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://iceberg-rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"warehouse\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.access-key\", \"admin\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.secret-key\", \"password\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.client.factory\", \"com.starrocks.connector.iceberg.IcebergAwsClientFactory\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Running\")\n",
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "wdi_data_df = spark.read.table(\"curated.world_development_indicators.data\")\n",
    "\n",
    "print(wdi_data_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n",
      "root\n",
      " |-- Country_Name: string (nullable = true)\n",
      " |-- Country_Code: string (nullable = true)\n",
      " |-- Indicator_Name: string (nullable = true)\n",
      " |-- Indicator_Code: string (nullable = true)\n",
      " |-- Indicator_Value: string (nullable = true)\n",
      " |-- year: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+---------------+----+\n",
      "|        Country_Name|Country_Code|      Indicator_Name|      Indicator_Code|Indicator_Value|year|\n",
      "+--------------------+------------+--------------------+--------------------+---------------+----+\n",
      "|Africa Eastern an...|         AFE|Access to clean f...|      EG.CFT.ACCS.ZS|           NULL|1960|\n",
      "|Africa Eastern an...|         AFE|Access to electri...|      EG.ELC.ACCS.ZS|           NULL|1960|\n",
      "|Africa Eastern an...|         AFE|Access to electri...|   EG.ELC.ACCS.RU.ZS|           NULL|1960|\n",
      "|Africa Eastern an...|         AFE|Access to electri...|   EG.ELC.ACCS.UR.ZS|           NULL|1960|\n",
      "|Africa Eastern an...|         AFE|Adjusted net nati...|   NY.ADJ.NNTY.KD.ZG|           NULL|1960|\n",
      "|Africa Eastern an...|         AFE|Adjusted net nati...|      NY.ADJ.NNTY.KD|           NULL|1960|\n",
      "|Africa Eastern an...|         AFE|Adjusted net nati...|      NY.ADJ.NNTY.CD|           NULL|1960|\n",
      "|Africa Eastern an...|         AFE|Adjusted net nati...|NY.ADJ.NNTY.PC.KD.ZG|           NULL|1960|\n",
      "|Africa Eastern an...|         AFE|Adjusted net nati...|   NY.ADJ.NNTY.PC.KD|           NULL|1960|\n",
      "|Africa Eastern an...|         AFE|Adjusted net nati...|   NY.ADJ.NNTY.PC.CD|           NULL|1960|\n",
      "+--------------------+------------+--------------------+--------------------+---------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "schema_wdi = StructType([\n",
    "               StructField('Country_Name', StringType(), True),\n",
    "               StructField('Country_Code', StringType(), True),\n",
    "               StructField('Indicator_Name', StringType(), True),\n",
    "               StructField('Indicator_Code', StringType(), True),\n",
    "               StructField('Indicator_Value', StringType(), True),\n",
    "               StructField('year', StringType(), True)\n",
    "             ])\n",
    "\n",
    "dataframes_list = []\n",
    "\n",
    "# Collect all DataFrames into a list\n",
    "for year in range(1960, 2021):\n",
    "    df_temp = (wdi_data_df\n",
    "               .select(\n",
    "                   'Country_Name',\n",
    "                   'Country_Code',\n",
    "                   'Indicator_Name',\n",
    "                   'Indicator_Code',\n",
    "                   F.col(str(year)).alias('Indicator_Value')\n",
    "               )\n",
    "               .withColumn('year', F.lit(year))\n",
    "              )\n",
    "    dataframes_list.append(df_temp)\n",
    "\n",
    "# Union all DataFrames at once\n",
    "df_wdi_data_unpivoted = dataframes_list[0]\n",
    "for df_temp in dataframes_list[1:]:\n",
    "    df_wdi_data_unpivoted = df_wdi_data_unpivoted.union(df_temp)\n",
    "\n",
    "print(df_wdi_data_unpivoted.rdd.getNumPartitions())\n",
    "\n",
    "df_wdi_data_unpivoted.printSchema()\n",
    "\n",
    "df_wdi_data_unpivoted.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write unpivoted dataframe to a new table partitioned by year\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS serving.world_development_indicators\")\n",
    "\n",
    "df_wdi_data_unpivoted.write \\\n",
    "    .partitionBy('year') \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .saveAsTable(name=\"serving.world_development_indicators.wdi_data_unpivoted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
