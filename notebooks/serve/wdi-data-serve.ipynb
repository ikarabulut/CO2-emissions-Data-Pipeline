{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Catalog Setup\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://iceberg-rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"warehouse\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.access-key\", \"admin\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.secret-key\", \"password\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.client.factory\", \"com.starrocks.connector.iceberg.IcebergAwsClientFactory\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Running\")\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "print(\"current catalog:\", spark.catalog.currentCatalog())\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_data_df = spark.read.table(\"curated.world_development_indicators.data\")\n",
    "\n",
    "print(wdi_data_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "schema_wdi = StructType([\n",
    "               StructField('Country_Name', StringType(), True),\n",
    "               StructField('Country_Code', StringType(), True),\n",
    "               StructField('Indicator_Name', StringType(), True),\n",
    "               StructField('Indicator_Code', StringType(), True),\n",
    "               StructField('Indicator_Value', StringType(), True),\n",
    "               StructField('year', StringType(), True)\n",
    "             ])\n",
    "\n",
    "emptyRDD              = spark.sparkContext.emptyRDD()\n",
    "df_wdi_data_unpivoted = spark.createDataFrame(emptyRDD,schema_wdi)\n",
    "\n",
    "# We loop through the years\n",
    "# And then add the data of each year to the unpivoted dataframe\n",
    "for year in range(1960, 2021):\n",
    "  df_temp = (wdi_data_df\n",
    "             .select(\n",
    "               'Country_Name',\n",
    "               'Country_Code', \n",
    "               'Indicator_Name', \n",
    "               'Indicator_Code',\n",
    "               # We keep the column of the current year in the loop\n",
    "               F.col(str(year)).alias('Indicator_Value')\n",
    "             )\n",
    "             .withColumn('year', F.lit(year)) # We add a column that contains the value of the year\n",
    "            )\n",
    "  # We append this year's data to the output dataframe via union()\n",
    "  df_wdi_data_unpivoted = df_wdi_data_unpivoted.union(df_temp)\n",
    "\n",
    "# Printing the number of partitions of the output dataframe\n",
    "print(df_wdi_data_unpivoted.rdd.getNumPartitions())\n",
    "\n",
    "df_wdi_data_unpivoted.printSchema()\n",
    "\n",
    "df_wdi_data_unpivoted.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write unpivoted dataframe to a new table partitioned by year\n",
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS wdi_serving\")\n",
    "df_wdi_data_unpivoted.createOrReplaceTempView(\"data_unpivoted_tempTable\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS wdi_serving.wdi_data_unpivoted \n",
    "  USING iceberg\n",
    "  PARTITIONED BY (year) \n",
    "  AS SELECT * FROM data_unpivoted_tempTable\n",
    "\"\"\")\n",
    "\n",
    "spark.catalog.dropTempView(\"data_unpivoted_tempTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use agg() method to perform aggregations\n",
    "# We use avg() from the pyspark.sql.functions module to generate the average\n",
    "# We apply the avg() function on a column from the grouped dataframe\n",
    "df_wdi_data_average = (df_wdi_data_unpivoted\n",
    "                       .groupBy(\n",
    "                         'Country_Name',\n",
    "                         'Country_Code', \n",
    "                         'Indicator_Name', \n",
    "                         'Indicator_Code',\n",
    "                       )\n",
    "                       .agg(\n",
    "                        F.avg('Indicator_Value').alias('Indicator_Average_Value')\n",
    "                       )\n",
    "                      )\n",
    "\n",
    "df_wdi_data_unpivoted.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the output data to the serving layer on DBFS\n",
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS wdi_serving\")\n",
    "repartitioned_df_wdi_data_average = df_wdi_data_average.repartition('Indicator_Code')\n",
    "\n",
    "(repartitioned_df_wdi_data_average\n",
    " .repartition(\"Indicator_Code\")\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .format('iceberg')\n",
    " .partitionBy('Indicator_Code')\n",
    " .saveAsTable('wdi_serving.partitioned_average_indicators')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
