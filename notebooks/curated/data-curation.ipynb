{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/18 01:05:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n",
      "[('spark.eventLog.enabled', 'true'), ('spark.driver.cores', '4'), ('spark.task.cpus', '4'), ('spark.app.submitTime', '1721264702368'), ('spark.executor.cores', '4'), ('spark.history.fs.logDirectory', '/home/iceberg/spark-events'), ('spark.sql.catalog.demo.s3.endpoint', 'http://minio:9000'), ('spark.eventLog.dir', '/home/iceberg/spark-events'), ('spark.app.id', 'local-1721264703143'), ('spark.driver.port', '36821'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.executor.memory', '8g'), ('spark.submit.deployMode', 'client'), ('spark.driver.host', '2f37682e1403'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalogImplementation', 'in-memory'), ('spark.sql.catalog.demo.warehouse', 's3://warehouse/wh/'), ('spark.sql.catalog.demo.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO'), ('spark.executor.id', 'driver'), ('spark.app.startTime', '1721264702538'), ('spark.app.name', 'PySparkShell'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.sql.catalog.demo.uri', 'http://rest:8181'), ('spark.sql.catalog.demo.type', 'rest'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalog.demo', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.defaultCatalog', 'demo'), ('spark.sql.warehouse.dir', 'file:/home/iceberg/notebooks/spark-warehouse'), ('spark.submit.pyFiles', ''), ('spark.ui.showConsoleProgress', 'true')]\n",
      "current catalog: demo\n",
      "Spark UI: http://2f37682e1403:4041\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Catalog Setup\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Running\")\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "print(\"current catalog:\", spark.catalog.currentCatalog())\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_data_df = spark.read.table(\"raw.world_development_indicators.WDIData\")\n",
    "wdi_country_df = spark.read.table(\"raw.world_development_indicators.WDICountry\")\n",
    "wdi_series_df = spark.read.table(\"raw.world_development_indicators.WDISeries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records for wdi data DF: 383838\n",
      "Number of records for wdi country DF: 270\n",
      "Number of records for wdi series DF: 4274\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of records for wdi data DF: {wdi_data_df.count()}\")\n",
    "print(f\"Number of records for wdi country DF: {wdi_country_df.count()}\")\n",
    "print(f\"Number of records for wdi series DF: {wdi_series_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Column names:: ['Country_Name', 'Country_Code', 'Indicator_Name', 'Indicator_Code', '1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\n",
      "Updated Column names:: ['Country_Code', 'Short_Name', 'Table_Name', 'Long_Name', '2-alpha_code', 'Currency_Unit', 'Special_Notes', 'Region', 'Income_Group', 'WB-2_code', 'National_accounts_base_year', 'National_accounts_reference_year', 'SNA_price_valuation', 'Lending_category', 'Other_groups', 'System_of_National_Accounts', 'Alternative_conversion_factor', 'PPP_survey_year', 'Balance_of_Payments_Manual_in_use', 'External_debt_Reporting_status', 'System_of_trade', 'Government_Accounting_concept', 'IMF_data_dissemination_standard', 'Latest_population_census', 'Latest_household_survey', 'Source_of_most_recent_Income_and_expenditure_data', 'Vital_registration_complete', 'Latest_agricultural_census', 'Latest_industrial_data', 'Latest_trade_data']\n",
      "Updated Column names:: ['Series_Code', 'Topic', 'Indicator_Name', 'Short_definition', 'Long_definition', 'Unit_of_measure', 'Periodicity', 'Base_Period', 'Other_notes', 'Aggregation_method', 'Limitations_and_exceptions', 'Notes_from_original_source', 'General_comments', 'Source', 'Statistical_concept_and_methodology', 'Development_relevance', 'Related_source_links', 'Other_web_links', 'Related_indicators', 'License_Type']\n"
     ]
    }
   ],
   "source": [
    "# Replace spaces in column names with underscores (“_”) for all DataFrames.\n",
    "\n",
    "# wdi_data \n",
    "wdi_data_columns = wdi_data_df.columns\n",
    "\n",
    "for column in wdi_data_columns:\n",
    "  if column.__contains__(\" \"):\n",
    "    new_column_name = column.replace(\" \", \"_\")\n",
    "    wdi_data_df = wdi_data_df.withColumnRenamed(column, new_column_name)\n",
    "\n",
    "print(f\"Updated Column names:: {wdi_data_df.columns}\")\n",
    "\n",
    "\n",
    "# wdi_country\n",
    "wdi_country_columns = wdi_country_df.columns\n",
    "\n",
    "for column in wdi_country_columns:\n",
    "  if column.__contains__(\" \"):\n",
    "    new_column_name = column.replace(\" \", \"_\")\n",
    "    wdi_country_df = wdi_country_df.withColumnRenamed(column, new_column_name)\n",
    "\n",
    "print(f\"Updated Column names:: {wdi_country_df.columns}\")\n",
    "\n",
    "# wdi_series\n",
    "wdi_series_columns = wdi_series_df.columns\n",
    "\n",
    "for column in wdi_series_columns:\n",
    "  if column.__contains__(\" \"):\n",
    "    new_column_name = column.replace(\" \", \"_\")\n",
    "    wdi_series_df = wdi_series_df.withColumnRenamed(column, new_column_name)\n",
    "\n",
    "print(f\"Updated Column names:: {wdi_series_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/18 01:05:08 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wdi data with null dropped count:: 280622\n",
      "Wdi country with null dropped count:: 270\n",
      "Wdi series with null dropped count:: 4274\n",
      "Wdi data with duplicates dropped count:: 280622\n",
      "Wdi country with duplicates dropped count:: 270\n",
      "Wdi series with duplicates dropped count:: 4274\n"
     ]
    }
   ],
   "source": [
    "# Drop records that only consist of null values (records with null values on all columns).\n",
    "\n",
    "year_columns = list(str(year) for year in range(1960, 2021))\n",
    "\n",
    "wdi_data_df = wdi_data_df.dropna(how=\"all\", subset=year_columns)\n",
    "wdi_country_df = wdi_country_df.dropna(how=\"all\")\n",
    "wdi_series_df = wdi_series_df.dropna(how=\"all\")\n",
    "\n",
    "print(f\"Wdi data with null dropped count:: {wdi_data_df.count()}\")\n",
    "print(f\"Wdi country with null dropped count:: {wdi_country_df.count()}\")\n",
    "print(f\"Wdi series with null dropped count:: {wdi_series_df.count()}\")\n",
    "\n",
    "# Drop duplicate records\n",
    "\n",
    "wdi_data = wdi_data_df.dropDuplicates()\n",
    "wdi_country = wdi_country_df.dropDuplicates()\n",
    "wdi_series = wdi_series_df.dropDuplicates()\n",
    "\n",
    "print(f\"Wdi data with duplicates dropped count:: {wdi_data_df.count()}\")\n",
    "print(f\"Wdi country with duplicates dropped count:: {wdi_country_df.count()}\")\n",
    "print(f\"Wdi series with duplicates dropped count:: {wdi_series_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdi country with filtered country code:: 265\n",
      "wdi data with filtered country code:: 280622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# For the WDICountry.csv and WDIData.csv files\n",
    "# Drop all records that have a country code (column: Country_Code) with a size other than three\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "wdi_country_df = wdi_country_df.filter(length(wdi_country.Country_Code) == 3)\n",
    "print(f\"wdi country with filtered country code:: {wdi_country_df.count()}\")\n",
    "wdi_data_df = wdi_data_df.filter(length(wdi_data_df.Country_Code) == 3)\n",
    "print(f\"wdi data with filtered country code:: {wdi_data_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdi series with filtered series code:: 1508\n"
     ]
    }
   ],
   "source": [
    "# For WDISeries.csv, drop all records that contain a space character (\" \") in the Series_Code column.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "wdi_series_df = wdi_series_df.filter(~col(\"Series_Code\").contains(\" \"))\n",
    "print(f\"wdi series with filtered series code:: {wdi_series_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(wdi_data\n",
    ".writeTo(\"curated.world_development_indicators.data\")\n",
    ".options(format=\"iceberg\", mode=\"overwrite\")\n",
    ".createOrReplace()\n",
    ")\n",
    "\n",
    "(wdi_country\n",
    ".writeTo(\"curated.world_development_indicators.country\")\n",
    ".options(format=\"iceberg\", mode=\"overwrite\")\n",
    ".createOrReplace()\n",
    ")\n",
    "\n",
    "(wdi_series\n",
    ".writeTo(\"curated.world_development_indicators.series\")\n",
    ".options(format=\"iceberg\", mode=\"overwrite\")\n",
    ".createOrReplace()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
