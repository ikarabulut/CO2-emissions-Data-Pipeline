{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n",
      "[('spark.eventLog.enabled', 'true'), ('spark.driver.port', '46609'), ('spark.driver.host', 'eaacf646f70f'), ('spark.history.fs.logDirectory', '/home/iceberg/spark-events'), ('spark.sql.catalog.demo.s3.endpoint', 'http://minio:9000'), ('spark.eventLog.dir', '/home/iceberg/spark-events'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.deployMode', 'client'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.sql.catalogImplementation', 'in-memory'), ('spark.sql.catalog.demo.warehouse', 's3://warehouse/wh/'), ('spark.sql.catalog.demo.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO'), ('spark.app.id', 'local-1720917814341'), ('spark.app.startTime', '1720917813571'), ('spark.executor.id', 'driver'), ('spark.app.name', 'PySparkShell'), ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'), ('spark.sql.catalog.demo.uri', 'http://rest:8181'), ('spark.sql.catalog.demo.type', 'rest'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.submitTime', '1720917813487'), ('spark.sql.defaultCatalog', 'demo'), ('spark.sql.catalog.demo', 'org.apache.iceberg.spark.SparkCatalog'), ('spark.sql.warehouse.dir', 'file:/home/iceberg/notebooks/spark-warehouse'), ('spark.submit.pyFiles', ''), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/14 00:43:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Catalog Setup\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://iceberg-rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"warehouse\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.access-key\", \"admin\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.secret-key\", \"password\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.client.factory\", \"com.starrocks.connector.iceberg.IcebergAwsClientFactory\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Running\")\n",
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- At1 (mm): long (nullable = true)\n",
      " |-- At2 (mm): long (nullable = true)\n",
      " |-- Cn: string (nullable = true)\n",
      " |-- Cr: string (nullable = true)\n",
      " |-- Ct: string (nullable = true)\n",
      " |-- De: string (nullable = true)\n",
      " |-- E (g/km): string (nullable = true)\n",
      " |-- Enedc (g/km): long (nullable = true)\n",
      " |-- Er (g/km): string (nullable = true)\n",
      " |-- Ernedc (g/km): double (nullable = true)\n",
      " |-- Erwltp (g/km): string (nullable = true)\n",
      " |-- Ewltp (g/km): long (nullable = true)\n",
      " |-- Fm: string (nullable = true)\n",
      " |-- Ft: string (nullable = true)\n",
      " |-- ID: long (nullable = true)\n",
      " |-- It: string (nullable = true)\n",
      " |-- MMS: string (nullable = true)\n",
      " |-- MS: string (nullable = true)\n",
      " |-- Man: string (nullable = true)\n",
      " |-- Mh: string (nullable = true)\n",
      " |-- Mk: string (nullable = true)\n",
      " |-- Mp: string (nullable = true)\n",
      " |-- Mt: long (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- T: string (nullable = true)\n",
      " |-- TAN: string (nullable = true)\n",
      " |-- VFN: string (nullable = true)\n",
      " |-- Va: string (nullable = true)\n",
      " |-- Ve: string (nullable = true)\n",
      " |-- Vf: string (nullable = true)\n",
      " |-- W (mm): long (nullable = true)\n",
      " |-- Zr: string (nullable = true)\n",
      " |-- ec (cm3): long (nullable = true)\n",
      " |-- ep (KW): long (nullable = true)\n",
      " |-- m (kg): long (nullable = true)\n",
      " |-- r: long (nullable = true)\n",
      " |-- version_file: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- z (Wh/km): long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/14 01:10:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------------------------+---+---+----+--------+------------+---------+-------------+-------------+------------+---+------+-------+---+--------------------------+---+-----------------------+-------+-------+-----------+----+------+----+------------------+---+----------+------------+----+------+----+--------+-------+------+---+------------+----+---------+\n",
      "|At1 (mm)|At2 (mm)|Cn                      |Cr |Ct |De  |E (g/km)|Enedc (g/km)|Er (g/km)|Ernedc (g/km)|Erwltp (g/km)|Ewltp (g/km)|Fm |Ft    |ID     |It |MMS                       |MS |Man                    |Mh     |Mk     |Mp         |Mt  |Status|T   |TAN               |VFN|Va        |Ve          |Vf  |W (mm)|Zr  |ec (cm3)|ep (KW)|m (kg)|r  |version_file|year|z (Wh/km)|\n",
      "+--------+--------+------------------------+---+---+----+--------+------------+---------+-------------+-------------+------------+---+------+-------+---+--------------------------+---+-----------------------+-------+-------+-----------+----+------+----+------------------+---+----------+------------+----+------+----+--------+-------+------+---+------------+----+---------+\n",
      "|1679    |1632    |458 SPECIALE A AD S-A   |   |M1 |NULL|NULL    |559         |NULL     |NULL         |NULL         |NULL        |M  |petrol|416839 |   |FERRARI                   |GB |FERRARI SPA            |FERRARI|FERRARI|           |NULL|F     |F142|e3*2007/46*0040*10|   |AB        |L           |NULL|2650  |NULL|4497    |NULL   |1485  |1  |v16         |2017|NULL     |\n",
      "|1679    |1632    |458 SPECIALE A AD S-A   |   |M1 |NULL|NULL    |559         |NULL     |NULL         |NULL         |NULL        |M  |petrol|416839 |   |FERRARI                   |GB |FERRARI SPA            |FERRARI|FERRARI|           |NULL|P     |F142|e3*2007/46*0040*10|   |AB        |L           |NULL|2650  |NULL|4497    |NULL   |1485  |1  |v15         |2017|NULL     |\n",
      "|NULL    |NULL    |AUDI A8                 |M1 |M1 |NULL|NULL    |545         |NULL     |NULL         |NULL         |NULL        |M  |PETROL|1360362|   |AUDI AG                   |DE |AA-IVA                 |AA-IVA |       |           |NULL|F     |F8  |                  |   |          |            |NULL|0     |NULL|3993    |420    |NULL  |4  |v16         |2017|NULL     |\n",
      "|NULL    |NULL    |AUDI A8                 |M1 |M1 |NULL|NULL    |545         |NULL     |NULL         |NULL         |NULL        |M  |PETROL|1360362|   |AUDI AG                   |DE |AA-IVA                 |AA-IVA |       |           |NULL|P     |F8  |                  |   |          |            |NULL|NULL  |NULL|3993    |420    |NULL  |4  |v15         |2017|NULL     |\n",
      "|1714    |1617    |BUGATTIGRANDSPORTVITESSE|M1 |M1 |NULL|NULL    |539         |NULL     |NULL         |NULL         |NULL        |M  |PETROL|1406058|   |BUGATTI (F)               |DE |BUGATTI AUTOMOBILES SAS|BUGATTI|BUGATTI|VW GROUP PC|NULL|P     |5B  |E1*KS07/46*0008*03|   |AEXCLBAX1 |AA7AD71C001S|NULL|2710  |NULL|7993    |882    |2065  |1  |v15         |2017|NULL     |\n",
      "|1714    |1617    |BUGATTIGRANDSPORTVITESSE|M1 |M1 |NULL|NULL    |539         |NULL     |NULL         |NULL         |NULL        |M  |PETROL|1406058|   |BUGATTI (F)               |DE |BUGATTI AUTOMOBILES SAS|BUGATTI|BUGATTI|VW GROUP PC|NULL|F     |5B  |E1*KS07/46*0008*03|   |AEXCLBAX1 |AA7AD71C001S|NULL|2710  |NULL|7993    |882    |2065  |1  |v16         |2017|NULL     |\n",
      "|1747    |1670    |UNKNOWN                 |   |M1 |NULL|NULL    |516         |NULL     |NULL         |NULL         |NULL        |M  |PETROL|423225 |   |BUGATTI                   |GB |BUGATTI AUTOMOBILES SAS|BUGATTI|BUGATTI|VW GROUP PC|NULL|F     |5B  |E1*KS07/46*0008*05|   |ADCNDALAX1|AD7AD71C002S|NULL|2710  |NULL|7993    |NULL   |2070  |1  |v16         |2017|NULL     |\n",
      "|1747    |1670    |UNKNOWN                 |   |M1 |NULL|NULL    |516         |NULL     |NULL         |NULL         |NULL        |M  |PETROL|426312 |   |BUGATTI                   |GB |BUGATTI AUTOMOBILES SAS|BUGATTI|BUGATTI|VW GROUP PC|NULL|F     |5B  |E1*KS07/46*0008*05|   |ADCNDALAX1|ADJAD716002S|NULL|2710  |NULL|7993    |NULL   |2070  |1  |v16         |2017|NULL     |\n",
      "|1747    |1670    |BUGATTICHIRON           |M1 |M1 |NULL|NULL    |516         |NULL     |NULL         |NULL         |NULL        |M  |PETROL|1406067|   |BUGATTI (F)               |DE |BUGATTI AUTOMOBILES SAS|BUGATTI|BUGATTI|VW GROUP PC|NULL|F     |5B  |E1*KS07/46*0008*05|   |ADCNDALAX1|AD7AD71C002S|NULL|2711  |NULL|7993    |1103   |2070  |7  |v16         |2017|NULL     |\n",
      "|1750    |1670    |BUGATTI CHIRON          |   |M1 |NULL|NULL    |516         |NULL     |NULL         |NULL         |NULL        |M  |PETROL|1581065|   |BUGATTI AUTOMOBILES S.A.S.|NL |AA-IVA                 |AA-IVA |BUGATTI|           |NULL|F     |5B  |e1*KS07/46*0008*05|   |ADCNDALAX1|AD7AD71C002S|NULL|2710  |NULL|7993    |1103   |2070  |1  |v16         |2017|NULL     |\n",
      "+--------+--------+------------------------+---+---+----+--------+------------+---------+-------------+-------------+------------+---+------+-------+---+--------------------------+---+-----------------------+-------+-------+-----------+----+------+----+------------------+---+----------+------------+----+------+----+--------+-------+------+---+------------+----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_co2_emissions_2017 = spark.read.table(\"raw.co2_passenger_cars_emissions.co2_emissions_passenger_cars_2017\")\n",
    "df_co2_emissions_2018 = spark.read.table(\"raw.co2_passenger_cars_emissions.co2_emissions_passenger_cars_2018\")\n",
    "df_co2_emissions_2019 = spark.read.table(\"raw.co2_passenger_cars_emissions.co2_emissions_passenger_cars_2019\")\n",
    "\n",
    "combined_co2_emissions_df = df_co2_emissions_2017.unionByName(df_co2_emissions_2018).unionByName(df_co2_emissions_2019)\n",
    "\n",
    "combined_co2_emissions_df.printSchema()\n",
    "combined_co2_emissions_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Column names:: ['At1 (mm)', 'At2 (mm)', 'Cn', 'Cr', 'Ct', 'De', 'E (g/km)', 'Enedc (g/km)', 'Er (g/km)', 'Ernedc (g/km)', 'Erwltp (g/km)', 'Ewltp (g/km)', 'Fm', 'Ft', 'ID', 'It', 'MMS', 'MS', 'Man', 'Mh', 'Mk', 'Mp', 'Mt', 'Status', 'T', 'TAN', 'VFN', 'Va', 'Ve', 'Vf', 'W (mm)', 'Zr', 'ec (cm3)', 'ep (KW)', 'm (kg)', 'r', 'version_file', 'year', 'z (Wh/km)']\n",
      "Updated Column names:: ['At1_mm', 'At2_mm', 'Cn', 'Cr', 'Ct', 'De', 'E_g/km', 'Enedc_g/km', 'Er_g/km', 'Ernedc_g/km', 'Erwltp_g/km', 'Ewltp_g/km', 'Fm', 'Ft', 'ID', 'It', 'MMS', 'MS', 'Man', 'Mh', 'Mk', 'Mp', 'Mt', 'Status', 'T', 'TAN', 'VFN', 'Va', 'Ve', 'Vf', 'W_mm', 'Zr', 'ec_cm3', 'ep_KW', 'm_kg', 'r', 'version_file', 'year', 'z_Wh/km']\n"
     ]
    }
   ],
   "source": [
    "# Replace spaces in column names with underscores (“_”). Additionally, remove parentheses from column names.\n",
    "import re\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "co2_emissions_columns = combined_co2_emissions_df.columns\n",
    "print(f\"Original Column names:: {combined_co2_emissions_df.columns}\")\n",
    "\n",
    "combined_co2_emissions_df = (combined_co2_emissions_df.select(\n",
    "                      [F.col(col).alias(re.sub('[()]', '', col.replace(' ', '_'))) for col in combined_co2_emissions_df.columns]\n",
    "                    ))\n",
    "\n",
    "\n",
    "print(f\"Updated Column names:: {combined_co2_emissions_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count prior to dropping null values:: 300000\n",
      "Record count after to dropping null values:: 300000\n"
     ]
    }
   ],
   "source": [
    "# Drop records that only consist of null values (records with null values on all columns).\n",
    "print(f\"Record count prior to dropping null values:: {combined_co2_emissions_df.count()}\")\n",
    "combined_co2_emissions_df = combined_co2_emissions_df.dropna(how=\"all\")\n",
    "print(f\"Record count after to dropping null values:: {combined_co2_emissions_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count prior to dropping duplicate values:: 300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count after to dropping duplicate values:: 300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Drop duplicate records.\n",
    "\n",
    "print(f\"Record count prior to dropping duplicate values:: {combined_co2_emissions_df.count()}\")\n",
    "combined_co2_emissions_df = combined_co2_emissions_df.dropDuplicates()\n",
    "print(f\"Record count after to dropping duplicate values:: {combined_co2_emissions_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count prior to filtered state code:: 300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count with filtered state code:: 299996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Drop all records that have a member state code size other than two (column: MS) and that contain any character other than uppercase letters in this column\n",
    "\n",
    "print(f\"Record count prior to filtered state code:: {combined_co2_emissions_df.count()}\")\n",
    "combined_co2_emissions_df = combined_co2_emissions_df.filter(combined_co2_emissions_df['MS'].rlike('^[A-Z][A-Z]$'))\n",
    "print(f\"Record count with filtered state code:: {combined_co2_emissions_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "current_year = datetime.now().year\n",
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS curated.co2_passenger_cars_emissions\")\n",
    "\n",
    "combined_co2_emissions_df = combined_co2_emissions_df.repartition('year')\n",
    "\n",
    "combined_co2_emissions_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy('year') \\\n",
    "    .saveAsTable(name=\"curated.co2_passenger_cars_emissions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
