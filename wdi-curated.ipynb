{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Development Indicators Data\n",
    "- ***WDIData.csv***: This is the main data file that contains the values of different indicators per country for years ranging from 1960 to 2020.\n",
    "- ***WDICountry.csv***: This file contains additional columns for each country.\n",
    "- ***WDISeries.csv***: This file contains additional columns for each series. (A series is basically a collection of data for a specific indicator and a specific set of countries over a period of time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csvs above into a spark dataframe\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "spark = DatabricksSession.builder.profile(\"DEFAULT\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_get = \"20240705\"\n",
    "\n",
    "wdi_data = (spark.read\n",
    "                                        .option('header', 'true')\n",
    "                                        .csv(f'dbfs:/Volumes/emissions_datapipeline_workspace/default/datalake/raw/world_development_indicators/date={date_to_get}/world_bank_data/WDIData.csv'))\n",
    "\n",
    "wdi_country = (spark.read\n",
    "                                        .option('header', 'true')\n",
    "                                        .csv(f'dbfs:/Volumes/emissions_datapipeline_workspace/default/datalake/raw/world_development_indicators/date={date_to_get}/world_bank_data/WDICountry.csv'))\n",
    "\n",
    "wdi_series = (spark.read\n",
    "                                        .option('header', 'true')\n",
    "                                        .csv(f'dbfs:/Volumes/emissions_datapipeline_workspace/default/datalake/raw/world_development_indicators/date={date_to_get}/world_bank_data/WDISeries.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records  for wdi data DF: 383838\n",
      "Number of records  for wdi country DF: 270\n",
      "Number of records  for wdi series DF: 4274\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of records  for wdi data DF: {wdi_data.count()}\")\n",
    "print(f\"Number of records  for wdi country DF: {wdi_country.count()}\")\n",
    "print(f\"Number of records  for wdi series DF: {wdi_series.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Column names:: ['Country_Name', 'Country_Code', 'Indicator_Name', 'Indicator_Code', '1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '_c65']\n",
      "Updated Column names:: ['Country_Code', 'Short_Name', 'Table_Name', 'Long_Name', '2-alpha_code', 'Currency_Unit', 'Special_Notes', 'Region', 'Income_Group', 'WB-2_code', 'National_accounts_base_year', 'National_accounts_reference_year', 'SNA_price_valuation', 'Lending_category', 'Other_groups', 'System_of_National_Accounts', 'Alternative_conversion_factor', 'PPP_survey_year', 'Balance_of_Payments_Manual_in_use', 'External_debt_Reporting_status', 'System_of_trade', 'Government_Accounting_concept', 'IMF_data_dissemination_standard', 'Latest_population_census', 'Latest_household_survey', 'Source_of_most_recent_Income_and_expenditure_data', 'Vital_registration_complete', 'Latest_agricultural_census', 'Latest_industrial_data', 'Latest_trade_data', '_c30']\n",
      "Updated Column names:: ['Series_Code', 'Topic', 'Indicator_Name', 'Short_definition', 'Long_definition', 'Unit_of_measure', 'Periodicity', 'Base_Period', 'Other_notes', 'Aggregation_method', 'Limitations_and_exceptions', 'Notes_from_original_source', 'General_comments', 'Source', 'Statistical_concept_and_methodology', 'Development_relevance', 'Related_source_links', 'Other_web_links', 'Related_indicators', 'License_Type', '_c20']\n"
     ]
    }
   ],
   "source": [
    "# Replace spaces in column names with underscores (“_”) for all DataFrames.\n",
    "\n",
    "# wdi_data \n",
    "wdi_data_columns = wdi_data.columns\n",
    "\n",
    "for column in wdi_data_columns:\n",
    "  if column.__contains__(\" \"):\n",
    "    new_column_name = column.replace(\" \", \"_\")\n",
    "    wdi_data = wdi_data.withColumnRenamed(column, new_column_name)\n",
    "\n",
    "print(f\"Updated Column names:: {wdi_data.columns}\")\n",
    "\n",
    "\n",
    "# wdi_country\n",
    "wdi_country_columns = wdi_country.columns\n",
    "\n",
    "for column in wdi_country_columns:\n",
    "  if column.__contains__(\" \"):\n",
    "    new_column_name = column.replace(\" \", \"_\")\n",
    "    wdi_country = wdi_country.withColumnRenamed(column, new_column_name)\n",
    "\n",
    "print(f\"Updated Column names:: {wdi_country.columns}\")\n",
    "\n",
    "# wdi_series\n",
    "wdi_series_columns = wdi_series.columns\n",
    "\n",
    "for column in wdi_series_columns:\n",
    "  if column.__contains__(\" \"):\n",
    "    new_column_name = column.replace(\" \", \"_\")\n",
    "    wdi_series = wdi_series.withColumnRenamed(column, new_column_name)\n",
    "\n",
    "print(f\"Updated Column names:: {wdi_series.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply data quality filters on the data for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wdi data with null dropped count:: 280622\n",
      "Wdi country with null dropped count:: 270\n",
      "Wdi series with null dropped count:: 4274\n",
      "Wdi data with duplicates dropped count:: 280622\n",
      "Wdi country with duplicates dropped count:: 270\n",
      "Wdi series with duplicates dropped count:: 2310\n"
     ]
    }
   ],
   "source": [
    "# Drop records that only consist of null values (records with null values on all columns).\n",
    "\n",
    "year_columns = list(str(year) for year in range(1960, 2021))\n",
    "\n",
    "wdi_data = wdi_data.dropna(how=\"all\", subset=year_columns)\n",
    "wdi_country = wdi_country.dropna(how=\"all\")\n",
    "wdi_series = wdi_series.dropna(how=\"all\")\n",
    "\n",
    "print(f\"Wdi data with null dropped count:: {wdi_data.count()}\")\n",
    "print(f\"Wdi country with null dropped count:: {wdi_country.count()}\")\n",
    "print(f\"Wdi series with null dropped count:: {wdi_series.count()}\")\n",
    "\n",
    "# Drop duplicate records\n",
    "\n",
    "wdi_data = wdi_data.dropDuplicates()\n",
    "wdi_country = wdi_country.dropDuplicates()\n",
    "wdi_series = wdi_series.dropDuplicates()\n",
    "\n",
    "print(f\"Wdi data with duplicates dropped count:: {wdi_data.count()}\")\n",
    "print(f\"Wdi country with duplicates dropped count:: {wdi_country.count()}\")\n",
    "print(f\"Wdi series with duplicates dropped count:: {wdi_series.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdi country with filtered country code:: 265\n",
      "wdi data with filtered country code:: 280622\n"
     ]
    }
   ],
   "source": [
    "# For the WDICountry.csv and WDIData.csv files\n",
    "# Drop all records that have a country code (column: Country_Code) with a size other than three\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "wdi_country = wdi_country.filter(length(wdi_country.Country_Code) == 3)\n",
    "print(f\"wdi country with filtered country code:: {wdi_country.count()}\")\n",
    "wdi_data = wdi_data.filter(length(wdi_data.Country_Code) == 3)\n",
    "print(f\"wdi data with filtered country code:: {wdi_data.count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdi series with filtered series code:: 1470\n"
     ]
    }
   ],
   "source": [
    "# For WDISeries.csv, drop all records that contain a space character (\" \") in the Series_Code column.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "wdi_series = wdi_series.filter(~col(\"Series_Code\").contains(\" \"))\n",
    "print(f\"wdi series with filtered series code:: {wdi_series.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the data to the data lake’s curated layer on DBFS (/datalake/curated/) under the following paths. The data should be in Parquet format and partitioned based on the current date, with one output file per partition\n",
    "\n",
    "Create the following external tables on top of the data:\n",
    "\n",
    "- wdi_curated.country for the countries DataFrame\n",
    "- wdi_curated.series for the series DataFrame\n",
    "- wdi_curated.data for the main DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_year = datetime.now().year\n",
    "current_month = datetime.now().month\n",
    "current_day = datetime.now().day\n",
    "\n",
    "dbfs_wdi_data_path = f\"dbfs:/Volumes/emissions_datapipeline_workspace/default/datalake/curated/world_development_indicators/data/year={current_year}/month={current_month}/day={current_day}/\"\n",
    "dbfs_wdi_series_path = f\"dbfs:/Volumes/emissions_datapipeline_workspace/default/datalake/curated/world_development_indicators/series/year={current_year}/month={current_month}/day={current_day}/\"\n",
    "dbfs_wdi_country_path = f\"dbfs:/Volumes/emissions_datapipeline_workspace/default/datalake/curated/world_development_indicators/country/year={current_year}/month={current_month}/day={current_day}/\"\n",
    "\n",
    "wdi_data.coalesce(1).write.mode(\"overwrite\").parquet(dbfs_wdi_data_path)\n",
    "wdi_series.coalesce(1).write.mode(\"overwrite\").parquet(dbfs_wdi_series_path)\n",
    "wdi_country.coalesce(1).write.mode(\"overwrite\").parquet(dbfs_wdi_country_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
